---
layout: default
title: "ARC-AGI"
date: 2025-03-26
---

# **ARC-AGI** is solvable.

# 0. The challenge

Compositional generalization is a key feature of human intelligence. Given a few pieces and a few ways of combining them, young children can entertain themselves endlessly building an infinite array of ever-more-complex combinations. Something inside every human being wants to combine things in new ways for the joy of witnessing the novel product. We do this like breathing.

LLMs (esp. non-reasoning models) do not handle compositional generalization well. For various architectural reasons (esp. attention), and the dynamics of SGD training with a next-token objecive, autoregressive transformers typically decline to learn compositional representations of their training data and instead elect to represent it iconically through several redundant memories of data points and attentions patterns. At inferrence time, these redundant representations are ensembled depending on the model's inferred task context.

This emergent method of information processing is highly data-inefficient, much in the way that a student who learns math by rote ultimately needs many more examples to learn any new application of the knowledge they have ostensibly acquired. It also prevents models from gaining the kind of self-knowledge that humans acquire when they deliberate on how to select and combine their mental skills in a novel context: LLMs don't know how they do what they do -- and until they can, they will necessarily be unable to gain the very human kind of self-knowledge regarding how much to trust that their own skills will function when combined in a new setting for deployment. Compositional generalization matters.

How can we measure this skill and improve it?

Operationalizations about. None is perfect. Some are useful. Some are useful and beautiful, for example, these three:

- ARC-AGI
- SRaven
- Szpakowksi concepts

Ultimately all such examples can be subsumed in math, situated as special cases of the larger sandboxes of patterns that professional mathematicians create -- and decoate, and expand -- for their recreation. While it is less aesthetic, then, I will also include BBH, a hard benchmark of mathematics reasoning problems.

# 1. Test-time training

Compositional generalization directly promotes sample efficiency; they are inseparable. Several approaches are possible to promote greater sample-efficiency:

- Metalearning and metacognition
- Transductive learning
- Extended "reasoning" (CoT)
- RL / curiosity-based methods
- Hypernetworks
- Reasoning models (test-time compute)
- ...

I'll revisit these idesas in section 2. For now, I want to describe an approach that used test-time training to achieves good results on the ARC-AGI dataset.

## 1.1

In *The Surprising Effectiveness of Test-Time Training* (https://arxiv.org/abs/2411.07279), the authors deploy a combination of technqiues well adapted to the structured, compositional, and data-scarce setting of ARC-AGI.

The idea of test-time training is to update the weights of the model by training on a dataset D_{TTT} of examples created at test-time. Thus, the core of the idea is how to much such a dataset.

To this end, it is useful to frame ARC-AGI as an [in-context learning (ICL)](https://functions.baulab.info) task. That is, each task in ARC-AGI is a collection of examples of input-output pairs, 

```
T_k = {(x_1, y_1), (x_2, y_2), ..., (x_Nk, y_Nk), y_{Nk+1}}
```

together with a final unpaired "test" input `y_{N_k+1}`. the goal is to successfully predict the output of the test input by inferring the single rule that maps every example input to its respective output -- and then to correctly apply that rule to the test input.

- The dataset: ICL -> test-time training
- 

- 

<img 
    src="/assets/images/ttt.png" 
    alt="Description" 
    style="
        width: 100%; 
        max-width: 800px; 
        height: auto; 
        display: block; 
        margin: 0 auto;
    "
/>


# 2. Dimensions of growth

