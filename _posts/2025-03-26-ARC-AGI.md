---
layout: default
title: "ARC-AGI"
date: 2025-03-26
---

# **ARC-AGI** is solvable.

# 0. The challenge

Compositional generalization is a key feature of human intelligence. Given a few pieces and a few ways of combining them, young children can entertain themselves endlessly building an infinite array of ever-more-complex combinations. Something inside every human being wants to combine things in new ways for the joy of witnessing the novel product. We do this like breathing.

LLMs (esp. non-reasoning models) do not handle compositional generalization well. For various architectural reasons (esp. attention), and the dynamics of SGD training with a next-token objecive, autoregressive transformers typically decline to learn compositional representations of their training data and instead elect to represent it iconically through several redundant memories of data points and attentions patterns. At inferrence time, these redundant representations are ensembled depending on the model's inferred task context.

This emergent method of information processing is highly data-inefficient, much in the way that a student who learns math by rote ultimately needs many more examples to learn any new application of the knowledge they have ostensibly acquired. It also prevents models from gaining the kind of self-knowledge that humans acquire when they deliberate on how to select and combine their mental skills in a novel context: LLMs don't know how they do what they do -- and until they can, they will necessarily be unable to gain the very human kind of self-knowledge regarding how much to trust that their own skills will function when combined in a new setting for deployment. Compositional generalization matters.

How can we measure this skill and improve it?

Operationalizations about. None is perfect. Some are useful. Some are useful and beautiful, for example, these three:

- ARC-AGI
- SRaven
- Szpakowksi concepts

Ultimately all such examples can be subsumed in math, situated as special cases of the larger sandboxes of patterns that professional mathematicians create -- and decoate, and expand -- for their recreation. While it is less aesthetic, then, I will also include BBH, a hard benchmark of mathematics reasoning problems.

# 1. Test-time training

Compositional generalization directly promotes sample efficiency; they are inseparable. Several approaches are possible to promote greater sample-efficiency:

- Metalearning and metacognition
- Transductive learning
- Extended "reasoning" (CoT)
- RL / curiosity-based methods
- Hypernetworks
- Reasoning models (test-time compute)
- ...

I'll revisit these idesas in section 2. For now, I want to describe an approach that used test-time training to achieves good results on the ARC-AGI dataset.

## 1.1

In *The Surprising Effectiveness of Test-Time Training* (https://arxiv.org/abs/2411.07279), the authors deploy a combination of technqiues well adapted to the structured, compositional, and data-scarce setting of ARC-AGI.

The idea of test-time training is to update the weights of the model by training on a dataset `D_{TTT}` of examples created at test-time -- in fact, this dataset will be made fresh and trained on any time the model is asked to make a new prediction. The core of method is how to much such a dataset (and train on it -- and infer from it).

To this end, it is useful to frame ARC-AGI as an [in-context learning (ICL)](https://functions.baulab.info) task. That is, each task in ARC-AGI is a collection of examples of input-output pairs, 

```
T_k = {(x_1, y_1), (x_2, y_2), ..., (x_Nk, y_Nk), y_{Nk+1}}
```

together with a final unpaired "test" input $x_{N_k+1}$. the goal is to successfully predict the output of the test input by inferring the single rule that maps every example input to its respective output -- and then to correctly apply that rule to the test input.

Here's one ARC task from the paper - the authors are happy that their methods improve on ICL baselines. By the way -- it's a nice rule, isn't it? How would you write a small program that implemented the rule's IO function $x \mapsto y$?

**dataset example**

<img 
    src="/assets/images/arc-example.png" 
    alt="Description" 
    style="
        width: 100%; 
        max-width: 800px; 
        height: auto; 
        display: block; 
        margin: 0 auto;
    "
/>

The core idea is that it is possible to train on the dataset of ICL examples for any ICL task. Given a task `T_k` with `N_k` IO pairs to demonstrate it, we can make a dataset of `N_k - 1` ICL tasks, where each task is just the original ICL problem but omitting one pair `(x_i, y_i)` and using `y_i` as the ground truth prediction target.

***training***
For each task:

- Make a dataset of examples using the leave-one-out idea.
  - Augment this dataset using invertible transformations: rotations, reflections, transpositions, color permutations, and input sequence permuations. (Note: maybe they should have used a [deep set](https://arxiv.org/abs/1703.06114) instead.)
- Minimize the CE loss on this dataset.
  - Note: for each example in the ICL dataset, they apply the loss to all the outputs, not only the output without ground truth in that batch.
  - Upate the parameters using a *different* LoRA adapter for each task.
- Do inference:
  - This is done with a two-stage hierarchical voting setup. Given a new example task with k demonstration IO pairs, a leave-one-out dataset is created. Each leave-one-out task with k sequences then has 1) each of the T transformations is applied, and 2) its sequence elements permuted randomly x2, for a total of 

``` 
n_predictions = k x T x 2 predictions
```

  - Then the *three* most frequent predicted outputs for each transformation advance to the second round of voting, from which the most frequent overall candidate is selected.

**Training pipeline**

<img 
    src="/assets/images/ttt.png" 
    alt="Description" 
    style="
        width: 100%; 
        max-width: 800px; 
        height: auto; 
        display: block; 
        margin: 0 auto;
    "
/>

***Details***

- The authors use Llama-3.2-1B, 3B, and 8B. All are the instruct finetuned models, and of course they use 8B for their final evaluations.
- **Important point:** The authors rely on extensive pretraining on a dataset of synthetic data created from programs that Michael Hodel wrote using a DSL he crafted specially for ARC. These [publicly available](https://github.com/michaelhodel/re-arc) programs are the correct and validated IO rules for ARC examples with ground truth solutions. Thus, these programs enables anyone to create every single valid IO pair for each ARC task. Moreover, by composing input-output functions for more than one task (and applying to larger input grids), one can even generate infinitely new ARC tasks, and generate (in principle) every valid IO pair for these as well. (See TTT appendix B, and Michael Hodel's paper [Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation](https://arxiv.org/abs/2404.07353).) ***Is this cheating?***

# 2. Dimensions of growth

> You might be wondering about all the other stuff. What about reasoning models that can blow ARC-AGI out of the water? I agree. Reasoning models (such as my favorite, R1) will keep doing RL for longer on ever richer verifiable corpora, and they might deliver ASI along that glide path.

But a good problem is worth obsessing over, and a useful tool is worth honing. We can use ARC-AGI to hone the transformer as well as its training paradigm.

---

**transformer modifications**

- Attention is shallow. Each attention head always seeks out identical patterns in input `x`. The actual shape of the attention pattern, which determines the spatial rearrangement of inputs, is entirely determined by the inputs, since

$$
a = (Kx)^{\intercal} Qx
$$

- In practice, this means that, if a novel input-output permutation is required, an attention head can only perform it if that permutation is enocded in a context vector that a prior MLP uses to bias the weights (*** clean up). The form of functional adaptation is actually very restricted, as implied by the architecture, and studied here.
- Attention is confused. See, e.g., Ethan Smith's [post](https://www.ethansmith2000.com/post/softmax-attention-is-a-fluke). The issue is that attention is normalized by softmax to sum to 1 along the query dimension. However, this forces it to give weight to tokens about which it may have no good information. Quite a lot of computation is devoted to downstream error correction, with e.g. inhibitory heads erasing the output of overactive upstream heads. ADD HYDRA ***Why can't attention heads do nothing?***
- Attention heads have limited awareness of their effect on other heads. Each head writes to the residual stream, but in novel contexts, multiple heads may write to the same dimensions of the stream and thereby confuse one another's messages. Which message should be preserved when two contexts both seem relevant and write an incomprehensible (failed) superposition of context vectors to the residual stream?
- It can be even better. These merge conflicts signal that perhaps, two relevant contexts have never coexisted. With appropriately expressive architecture and sensitive training, this provides an opportunity to learn 1) which head should get priority, 2) how heads can work together, 3) how a compatible mutual representation can be formed in the residual stream.

**training modifications** 

- **Generative Training** The biggest issue with training is that it is not generative. If we are to do TTT, we should train models to produce more examples that they think they can learn from. In particular:
  - We should not need to manually supply primitive invertible transformations such as rotations, reflections, and rescalings.
  - We should not need to augment the dataset with additional examples we create.
  - We should not need to create D_{TTT} through our own algorithm merely to feed it to the model; the model itself should be able to figure out which sorts of questions similar to the task at hand it can ask and, with effort, answer.
- **World-model curiosity** It should be curious about the world. Which rule should I investigate to help me solve this problem? And how would learning it help me solve other problems I've seen? This is a form of self-guided **curriculum learning** implemented with curiosity.
- It should be curious about itself. Which parts of my knowledge seem like they'll be useful here, despite the missing pieces? How can I choose to work on problems that will leave me better informed about **my own skills and learning style**?
- Program synthesis: given *any* rule that is *truly* understood, *a model ought to be able to generate at least one unique example of a valid IO pair for **every single natural number*** $n \in \mathbb{N} = \{1, 2, 3, ...\}$. Surely this is enough training data. It's only ... what constitutes rules that are worth generating this way and thereby committing to memory? Perhaps it is determined only by the actual program ability.
- Flexibility and program compositionality: What should we do when we get stuck? Maybe another problem has become easier. We could finally do this via a multi-armed bandit approach because the number of tasks is discrete. Essentially, we want to revisit tasks in order of how much we think we can learn by trying them again with the knowledge we now have:
```
T* = argmax_T I(W | T, K_{now})
```
- T* could be the current task.